---
title: 'Practical Machine Learning: Course Project'
author: "Alejandro Osorio"
date: "August, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Getting and Cleaning Data

Training and testing sets were downloaded using read_csv function, with the 'na' parameter tweaked so that it also included "#DIV/0!" cases (besides the default "" and "NA" cases).  Additionally, first columns containing row numbers were eliminated from the obtained datasets.

```{r, include=FALSE}
training <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na = c("", "NA", "#DIV/0!"))
training <- training[,-1]
testing <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na = c("", "NA"))
testing <- testing[,-1]
```

Next, columns from the testing set that included NAs, where identified and dimensioned the following way:

```{r}
table(colSums((is.na(testing))))
```

Therefore, as all columns with NAs (100 in this case) had 100% NAs (20 each), those columns were removed both from the testing and training datasets.

```{r}
datacols <- names(which(colSums(is.na(testing))!=0))
testing <- select(testing, -datacols)
training <- select(training, -datacols)
```

Then, a final row check for NAs was performed for the training set

```{r}
which(colSums(is.na(training)) != 0)
which(rowSums(is.na(training)) != 0)
```

Finally, after removing the only row with the 3 remaining NAs from the training set, the obtained working set is checked for NAs for the last time:

```{r}
workingset <- training[-which(rowSums(is.na(training)) != 0),]
anyNA(workingset)
```

The final working dataset, with no NAs, went down to the following dimensions (detailed structure of the obtained dataset can be seen in Appendix 1):

```{r}
dim(workingset)
```


## Methodology

### Model Selection

Given the classification problem, the following model types were tried separately, each with its own feature selection.

1. GLM - Logistic Regression
2. Random Forests
3. Boosting

After obtaining results for each model, a final GAM model based on the combination of the previous ones was also tested.

### Error Type Selection

Given the classification problem, with more than two possible outcomes, the main error type used for each model was Accuracy.  Additionally, for the GAM model evaluation, RMSE was also taken into consideration.

### Cross Validation

The following three sets were created, using Random Subsampling:

1. Building Data Set: 70% of the working dataset.  This set was sub divided again into:
 * Training Set (70%)
 * Testing Set (30%)
2. Validation Set: 30% of the working dataset.

## Appendix 1: Getting and Cleaning Data

Final structure of the obtained training dataset (very similar to that of the testing set, but for the last column):

```{r}
str(workingset)
```
