---
title: 'Practical Machine Learning: Course Project'
author: "Alejandro Osorio"
date: "August, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
set.seed(123)
```

## Methodology

### Model Selection

Given the multiple-outcome-classification problem (more than 2 possible factor-type outcomes), the following model types were tried separately, each with its own feature selection.

1. Penalized Multinomial Logistic Regression
2. Random Forests
3. Boosting with Trees

After obtaining results for each model, a final GAM model based on the combination of the previous ones was also tested.

### Error Type Selection

Given the problem has more than two possible outcomes, the main error type used for each model was Accuracy.  Additionally, for the GAM model evaluation, RMSE was also taken into consideration.

### Data Splitting and Cross Validation

The following three sets were created, using Random Subsampling:

1. Building Data Set: 70% of the working dataset.  This set was sub divided again into:
 * Training Set (70%)
 * Testing Set (30%)
2. Validation Set: 30% of the working dataset.

Given the types of models selected (besides GLM), which use sub sampling themselves, all models were trained using the same training and testing sets.  The validation set was finally used for the GAM based combination model.

## Getting and Cleaning Data

Training and testing sets were downloaded using read_csv function, with the 'na' parameter tweaked so that it also included "#DIV/0!" cases (besides the default "" and "NA" cases).  Additionally, first columns containing row numbers were eliminated from the obtained datasets.

```{r, include=FALSE}
origtrain <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na = c("", "NA", "#DIV/0!"))
origtrain <- origtrain[,-1]
origtest <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na = c("", "NA"))
origtest <- origtest[,-1]
```

Next, columns from the testing set that included NAs, where identified and dimensioned the following way:

```{r}
table(colSums((is.na(origtest))))
```

Therefore, as all columns with NAs (100 in this case) had 100% NAs (20 each), those columns were removed both from the testing and training datasets.

```{r}
datacols <- names(which(colSums(is.na(origtest))!=0))
origtest <- select(origtest, -datacols)
origtrain <- select(origtrain, -datacols)
```

Then, a final row check for NAs was performed for the training set

```{r}
which(colSums(is.na(origtrain)) != 0)
which(rowSums(is.na(origtrain)) != 0)
```

Finally, after removing the only row with the 3 remaining NAs from the training set, the obtained working set is checked for NAs for the last time:

```{r}
workingset <- origtrain[-which(rowSums(is.na(origtrain)) != 0),]
anyNA(workingset)
```

The final working dataset, with no NAs, went down to the following dimensions (detailed structure of the obtained dataset can be seen in Appendix 1):

```{r}
dim(workingset)
```

## Data Splitting

Step 1: Creating building and validation sets

```{r}
indbuild <- createDataPartition(workingset$classe, p = 0.7, list = FALSE)
validata <- workingset[-indbuild,]
buildata <- workingset[indbuild,]
```

Step 2: Creating training and testing sets

```{r}
indtrain <- createDataPartition(buildata$classe, p = 0.7, list = FALSE)
training <- buildata[indtrain,]
testing <- buildata[-indtrain,]
```

## Model Testing

### Penalized Multinomial Logistic Regression

#### Training

```{r, include=FALSE}
modfitmlog <- train(classe ~ ., method = "multinom", data = training[,-c(1:6)])
```

Model used:

```{r, echo=FALSE}
modfitmlog$call
```

```{r, echo=FALSE}
predmlog <- predict(modfitmlog, newdata = testing[,-c(1:6, 59)])
```

Overall Results:

```{r, echo=FALSE}
confmatmlog <- confusionMatrix(predmlog, factor(testing$classe))
confmatmlog$table
confmatmlog$overall
```

#### P-Values for preliminary feature selection

t Statistics:

Given the fact that null hypotheses consider coefficients equal to cero, the t statistics are just the estimates divided by their standard errors, as follows:

```{r}
modfitmlog_coef <- summary(modfitmlog)$coefficients
modfitmlog_stderr <- summary(modfitmlog)$standard.errors
tBetas <- modfitmlog_coef/modfitmlog_stderr
```

Degrees of freedom:

```{r}
# Model's effective degrees of freedom
modfitmlog_edf <- modfitmlog$finalModel["edf"]

# Degrees of freedom for p-Values:
n <- length(training$classe)
df <- n - modfitmlog_edf[[1]]
df
```

p Values:
```{r}
pBetas <- 2 * data.frame(pt(abs(tBetas), df = df, lower.tail = FALSE))
```

#### Preliminary feature selection

Features eliminated due to high p-values for all possible outcome factors:
```{r}
lesscoeff <- select(pBetas, contains("gyros"))
names(lesscoeff)
```

#### Simplified model

```{r, include=FALSE}
namelist <- names(training[, -c(1:6, 59)])
lessfeatures <- reformulate(termlabels = namelist[-grep("gyro", namelist, value = FALSE)], response = 'classe')
modfitmlog2 <- train(lessfeatures, method = "multinom", data = training[, -c(1:6)])
predmlog2 <- predict(modfitmlog2, newdata = select(testing[,-c(1:6, 59)], -contains("gyros")))
confmatmlog2 <- confusionMatrix(predmlog2, factor(testing$classe))
confmatmlog2$table
confmatmlog2$overall
```

Interesting to note that almost the same performance was obtained with 12 less features than the original model.

#### ANOVA Analysis for final feature selection

PENDIENTE

### Predicting with Trees

```{r, include=FALSE}
modfittree <- train(classe ~ ., method = "rpart", data = training[,-c(1:6)])
predtree <- predict(modfittree, newdata = testing[,-c(1:6, 59)])
```

#### Model trained

```{r, echo=FALSE}
modfittree$call
```

#### Performance obtained

```{r}
confmattree <- confusionMatrix(predtree, factor(testing$classe))
confmattree$table
confmattree$overall
```

### Random Forest

Due to large processing time, training was carried on with less features (the ones obtained from Multinomial Logistic Regression based on p-value analysis of its coefficients).

```{r, include=FALSE}
modfitrf <- train(lessfeatures, method = "rf", data = training[,-c(1:6)])
predrf <- predict(modfitrf, newdata = testing[,-c(1:6, 59)])
```

#### Model trained

```{r, echo=FALSE}
modfitrf$call
```

#### Performance obtained

```{r}
confmatrf <- confusionMatrix(predrf, factor(testing$classe))
confmatrf$table
confmatrf$overall
```

### Boosting With Trees

```{r, include=FALSE}
modfitgbm <- train(classe ~ ., method = "gbm", data = training[,-c(1:6)])
predgbm <- predict(modfitgbm, newdata = testing[,-c(1:6, 59)])
```

#### Model trained

```{r, echo=FALSE}
modfitgbm$call
```

#### Performance Obtained

```{r}
confmatgbm <- confusionMatrix(predgbm, factor(testing$classe))
confmatgbm$table
confmatgbm$overall
```

## Appendix 1: Getting and Cleaning Data

Final structure of the obtained training dataset (very similar to that of the testing set, but for the last column):

```{r}
str(workingset)
```
