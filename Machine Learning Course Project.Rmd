---
title: 'Practical Machine Learning: Course Project'
author: "Alejandro Osorio"
date: "August, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
set.seed(123)
```

## Methodology

### Model Selection

Given the multiple-outcome-classification problem (more than 2 possible factor-type outcomes), the following model types were tried separately.

1. Penalized Multinomial Logistic Regression
2. Trees
3. Random Forests
4. Boosting with Trees

After obtaining results for each model, a final GAM model based on the combination of two of the previous ones (the ones with best accuracy) was also tested.

### Error Type Selection

Given the problem has more than two possible outcomes, the main error type used for each model was Accuracy (below 90% would be considered low).

### Feature Selection

Based on a p-value analysis on the coefficients of model 1 (Penalized Multinomial Logistic Regression), the final regressors were obtained for the rest of the models.

### Data Splitting and Cross Validation

The following three sets were created, using Random Subsampling:

1. Building Data Set: 70% of the working dataset.  This set was sub divided again into:
 * Training Set (70%)
 * Testing Set (30%)
2. Validation Set: 30% of the working dataset.

The validation set was finally used for the GAM based combination model.

## Getting and Cleaning Data

Training and testing sets were downloaded using read_csv function, with the 'na' parameter tweaked so that it also included "#DIV/0!" cases (besides the default "" and "NA" cases).  Additionally, first columns containing row numbers were eliminated from the obtained datasets.

```{r, include=FALSE}
origtrain <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na = c("", "NA", "#DIV/0!"))
origtrain <- origtrain[,-1]
origtest <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na = c("", "NA"))
origtest <- origtest[,-1]
```

Next, columns from the testing set that included NAs, where identified and dimensioned the following way:

```{r}
table(colSums((is.na(origtest))))
```

Therefore, as all columns with NAs (100 in this case) had 100% NAs (20 each), those columns were removed both from the testing and training datasets.

```{r}
datacols <- names(which(colSums(is.na(origtest))!=0))
origtest <- select(origtest, -datacols)
origtrain <- select(origtrain, -datacols)
```

Then, a final row check for NAs was performed for the training set

```{r}
which(colSums(is.na(origtrain)) != 0)
which(rowSums(is.na(origtrain)) != 0)
```

Finally, after removing the only row with the 3 remaining NAs from the training set, the obtained working set is checked for NAs for the last time:

```{r}
workingset <- origtrain[-which(rowSums(is.na(origtrain)) != 0),]
anyNA(workingset)
```

The final working dataset, with no NAs, went down to the following dimensions (detailed structure of the obtained dataset can be seen in Appendix 1):

```{r}
dim(workingset)
```

## Data Splitting

Step 1: Creating building and validation sets

```{r}
indbuild <- createDataPartition(workingset$classe, p = 0.7, list = FALSE)
validata <- workingset[-indbuild,]
buildata <- workingset[indbuild,]
```

Step 2: Creating training and testing sets

```{r}
indtrain <- createDataPartition(buildata$classe, p = 0.7, list = FALSE)
training <- buildata[indtrain,]
testing <- buildata[-indtrain,]
```

## Selection of Regressors and Model Testing

### Penalized Multinomial Logistic Regression

#### Training

```{r, include=FALSE}
modfitmlog <- train(classe ~ ., method = "multinom", data = training[,-c(1:6)])
```

Model used:

```{r, echo=FALSE}
modfitmlog$call
```

```{r, echo=FALSE}
predmlog <- predict(modfitmlog, newdata = testing[,-c(1:6, 59)])
```

Overall Results:

```{r, echo=FALSE}
confmatmlog <- confusionMatrix(predmlog, factor(testing$classe))
confmatmlog$table
confmatmlog$overall
```

Even though the obtained accuracy was considered low (below 90%), this model's main objective was to determine the regressors to use with the rest of models.  Therefore the following p-value analysis.

#### P-Values for feature selection

t Statistics:

Given the fact that null hypotheses consider coefficients equal to cero, the t statistics are just the estimates divided by their standard errors, as follows:

```{r}
modfitmlog_coef <- summary(modfitmlog)$coefficients
modfitmlog_stderr <- summary(modfitmlog)$standard.errors
tBetas <- modfitmlog_coef/modfitmlog_stderr
```

Degrees of freedom:

```{r}
# Model's effective degrees of freedom
modfitmlog_edf <- modfitmlog$finalModel["edf"]

# Degrees of freedom for p-Values:
n <- length(training$classe)
df <- n - modfitmlog_edf[[1]]
df
```

p Values:
```{r}
pBetas <- 2 * data.frame(pt(abs(tBetas), df = df, lower.tail = FALSE))
```

#### Preliminary feature selection

Features eliminated due to high p-values for all possible outcome factors:
```{r}
lesscoeff <- select(pBetas, contains("gyros"))
names(lesscoeff)
```

#### Model with final regressors

```{r, include=FALSE}
namelist <- names(training[, -c(1:6, 59)])
lessfeatures <- reformulate(termlabels = namelist[-grep("gyro", namelist, value = FALSE)], response = 'classe')
modfitmlog2 <- train(lessfeatures, method = "multinom", data = training[, -c(1:6)])
predmlog2 <- predict(modfitmlog2, newdata = select(testing[,-c(1:6, 59)], -contains("gyros")))
confmatmlog2 <- confusionMatrix(predmlog2, factor(testing$classe))
confmatmlog2$table
confmatmlog2$overall
```

Interesting to note that almost the same performance was obtained with 12 less features than the original model.  Therefore the remaining models will consider those regressors.

### Predicting with Trees

```{r, include=FALSE}
modfittree <- train(classe ~ ., method = "rpart", data = training[,-c(1:6)])
predtree <- predict(modfittree, newdata = testing[,-c(1:6, 59)])
```

#### Model trained

Given the quick result obtained with all regressors, there was no need to run the model with the subset obtained through model 1.

```{r, echo=FALSE}
modfittree$call
```

#### Performance obtained

```{r}
confmattree <- confusionMatrix(predtree, factor(testing$classe))
confmattree$table
confmattree$overall
```

It's worth noting that the Multinomial Logistic Regression model was almost ten percentage points more accurate than this one, even with less regressors.

### Random Forest

```{r, include=FALSE}
modfitrf <- train(lessfeatures, method = "rf", data = training[,-c(1:6)])
predrf <- predict(modfitrf, newdata = testing[,-c(1:6, 59)])
```

#### Model trained

Due to large processing time, training was carried on with the subset of features obtained from model 1 (Multinomial Logistic Regression based on p-value analysis of its coefficients).

```{r, echo=FALSE}
modfitrf$call
```

#### Performance obtained

```{r}
confmatrf <- confusionMatrix(predrf, factor(testing$classe))
confmatrf$table
confmatrf$overall
```

### Boosting With Trees

```{r, include=FALSE}
modfitgbm <- train(classe ~ ., method = "gbm", data = training[,-c(1:6)])
predgbm <- predict(modfitgbm, newdata = testing[,-c(1:6, 59)])
```

#### Model trained

In this case, given the brief time required to obtain the result, all possible features were used.

```{r, echo=FALSE}
modfitgbm$call
```

#### Performance obtained

```{r}
confmatgbm <- confusionMatrix(predgbm, factor(testing$classe))
confmatgbm$table
confmatgbm$overall
```

It's worth noting that Random Forest with less features obtained a better accuracy than this model, even though it contained all possible features.

### Combining Predictors

Based on the accuracy obtained with each previous model, Random Forests and Boosting with Trees were chosen for this stage.

#### Training data set

```{r}
dfpredcomb <- data.frame(predrf, predgbm, classe = factor(testing$classe))
```

The following data set for GAM training was obtained from each model's predictions ('rf' and 'gbm') plus the real results in the testing set ('real').

```{r, echo=FALSE}
str(dfpredcomb)
```

#### Model trained

```{r, include=FALSE}
modfitcomb <- train(classe ~ ., method = "gam", data = dfpredcomb)
predcomb <- predict(modfitcomb, dfpredcomb[,-3])
```

```{r, echo=FALSE}
modfitcomb$call
```

#### Performance obtained

```{r}
confmatcomb <- confusionMatrix(predcomb, factor(testing$classe))
confmatcomb$table
confmatcomb$overall
```

The warning message "fitted probabilities numerically 0 or 1 occurred" was generated for all combinations of predictors, besides de two presented here.  Additionally, it happened with methods 'gamLoess' and 'gamSpline'.  After some research, it means the function didn't converge due to one of the predictor variables separating perfectly the values of the dependent variable.

Final model to be used, then, will be Random Forest, with an accuracy of almost 99%.

### Testing Random Forest accuracy with Validation Data

```{r, echo=FALSE}
predrf_val <- predict(modfitrf, newdata = validata[,-c(1:6, 59)])
```

```{r}
confmatrf_val <- confusionMatrix(predrf_val, factor(validata$classe))
confmatrf_val$table
confmatrf_val$overall
```

Good to see an accuracy of almost 99% is reached again.  Good odds for the quiz!

## Appendix 1: Getting and Cleaning Data

Final structure of the obtained training dataset (very similar to that of the testing set, but for the last column):

```{r}
str(workingset)
```
